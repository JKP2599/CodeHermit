Metadata-Version: 2.4
Name: local-code-assistant
Version: 0.1.0
Summary: Your project description here
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: fastapi<1.0.0,>=0.109.0
Requires-Dist: uvicorn<1.0.0,>=0.27.0
Requires-Dist: crewai<1.0.0,>=0.11.0
Requires-Dist: autogen<1.0.0,>=0.2.0
Requires-Dist: chromadb<1.0.0,>=0.4.22
Requires-Dist: python-dotenv<2.0.0,>=1.0.0
Requires-Dist: psutil<6.0.0,>=5.9.8
Requires-Dist: pydantic-settings<3.0.0,>=2.0.0
Requires-Dist: streamlit<2.0.0,>=1.32.0
Requires-Dist: requests<3.0.0,>=2.31.0

# Local Code Assistant

A VSCode extension that provides AI-powered code assistance using CrewAI, AutoGen, and Ollama for local model inference.

## Features

- Code generation with context awareness
- Multi-agent code review using AutoGen
- Interactive chat interface
- Inline code completions
- Local model inference via Ollama
- Workspace context management with ChromaDB

## Prerequisites

### Common Requirements
- Python 3.10 or higher
- Node.js 18 or higher
- Git
- VSCode
- CUDA support (optional, for GPU acceleration)

### Platform-Specific Requirements
- **Windows**: WSL2 (Windows Subsystem for Linux 2)
- **Linux**: Standard development tools
- **macOS**: Xcode Command Line Tools

## Installation

### 1. Clone the Repository
```bash
git clone https://github.com/yourusername/local-code-assistant.git
cd local-code-assistant
```

### 2. Python Environment Setup

#### Windows (WSL2)
```bash
# Install Python 3.10+ if not already installed
sudo apt update
sudo apt install python3.10 python3.10-venv python3-pip

# Install uv package manager
curl -LsSf https://astral.sh/uv/install.sh | sh

# Create and activate virtual environment
uv venv
source .venv/bin/activate

# Install dependencies
uv pip install fastapi uvicorn crewai autogen chromadb python-dotenv psutil
```

#### Linux
```bash
# Install Python 3.10+ if not already installed
sudo apt update
sudo apt install python3.10 python3.10-venv python3-pip

# Install uv package manager
curl -LsSf https://astral.sh/uv/install.sh | sh

# Create and activate virtual environment
uv venv
source .venv/bin/activate

# Install dependencies
uv pip install fastapi uvicorn crewai autogen chromadb python-dotenv psutil
```

#### macOS
```bash
# Install Python 3.10+ using Homebrew
brew install python@3.10

# Install uv package manager
curl -LsSf https://astral.sh/uv/install.sh | sh

# Create and activate virtual environment
uv venv
source .venv/bin/activate

# Install dependencies
uv pip install fastapi uvicorn crewai autogen chromadb python-dotenv psutil
```

### 3. Install Ollama

#### Windows (WSL2)
```bash
# Install Ollama
curl https://ollama.ai/install.sh | sh

# Add Ollama to PATH
echo 'export PATH="$PATH:$HOME/.local/bin"' >> ~/.bashrc
source ~/.bashrc

# Pull CodeLlama model
ollama pull codellama:7b-instruct
```

#### Linux
```bash
# Install Ollama
curl https://ollama.ai/install.sh | sh

# Add Ollama to PATH
echo 'export PATH="$PATH:$HOME/.local/bin"' >> ~/.bashrc
source ~/.bashrc

# Pull CodeLlama model
ollama pull codellama:7b-instruct
```

#### macOS
```bash
# Install Ollama using Homebrew
brew install ollama

# Start Ollama service
ollama serve

# In a new terminal, pull CodeLlama model
ollama pull codellama:7b-instruct
```

### 4. Install VSCode Extension Dependencies

#### All Platforms
```bash
# Install Node.js dependencies
cd extension
npm install

# Build the extension
npm run build

# Package the extension
vsce package

# Install the extension in VSCode
code --install-extension local-code-assistant-ext-0.1.0.vsix
```

### 5. Configuration

#### All Platforms
1. Create environment file:
```bash
cp server/.env.template server/.env
```

2. Edit `server/.env` with your settings:
```env
OLLAMA_MODEL=codellama:7b-instruct
UV_PORT=8000
EMBEDDING_MODEL=codellama:7b-instruct
METRICS_ENABLED=true
```

## Usage

### 1. Start the Server

#### All Platforms
```bash
# Activate virtual environment if not already activated
source .venv/bin/activate

# Start the server
cd server
uv run main.py
```

### 2. Using the Extension in VSCode

1. Open VSCode
2. Open the Command Palette:
   - Windows/Linux: `Ctrl+Shift+P`
   - macOS: `Cmd+Shift+P`
3. Type "Code Assistant" to see available commands:
   - "Generate Code": Get AI suggestions for selected code
   - "Review Code": Run multi-agent code review
   - "Chat": Start interactive assistance

## Testing

### Test API Endpoints

#### All Platforms
```bash
# Generate code
curl -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt":"# hello world"}'

# Review code
curl -X POST http://localhost:8000/review \
  -H "Content-Type: application/json" \
  -d '{"prompt":"def example(): pass"}'

# Chat
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"message":"How do I write a Python decorator?"}'

# Get metrics
curl http://localhost:8000/metrics
```

## Troubleshooting

### Common Issues

1. **Ollama Connection Issues**
   - Ensure Ollama service is running
   - Check if port 11434 is available
   - Verify model is downloaded: `ollama list`

2. **Python Environment Issues**
   - Verify Python version: `python --version`
   - Ensure virtual environment is activated
   - Check dependencies: `uv pip list`

3. **VSCode Extension Issues**
   - Reload VSCode window
   - Check extension logs: `Help > Toggle Developer Tools`
   - Verify extension is installed: `code --list-extensions`

4. **Server Connection Issues**
   - Check if port 8000 is available
   - Verify server is running: `curl http://localhost:8000/metrics`
   - Check server logs for errors

## Architecture

- FastAPI backend with CrewAI flows
- AutoGen for multi-agent interactions
- ChromaDB for context management
- VSCode extension with Language Server Protocol
- Ollama for local model inference

## Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

## License

MIT License - see LICENSE file for details 
